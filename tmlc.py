# -*- coding: utf-8 -*-
"""TMLC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18l9ME-wR84rV9DoEctblT9BMU7TRyQyJ
"""

##importing Libraries

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv("/content/NIR Data_V4_noperc_for_plot.csv")
df.head()

df.isnull().sum()

df.dtypes

##EXPLORE - Find Significant Patterns and Trends using Statistical Method.

df.describe()

plt.figure(figsize=(5, 5))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True)
plt.show()

## MODEL - Construct Model to Predict and Forecast

##Split Data to Predictors and Target

X=df.drop("ContentUniformity",axis=1)
X.head()

y=df["ContentUniformity"]
y.head()

n_cols = X.shape[1]
n_cols

##Importing SKLEARN and KERAS Libraries

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score

import keras

from keras.models import Sequential
from keras.layers import Dense

#Building the Model

#Network Properties:(Hidden Layer: 1,Nodes: 10,Activation Function: ReLU,Optimizer: Adam,Loss Function: Mean absolute Error,Epochs: 50)

mae_A = []
r2_A = []

for i in range(50):
    
    #Split Data to Train and Test Set
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)

    #Create model
    model = Sequential()
    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))
    model.add(Dense(1))

    #Compile model
    model.compile(optimizer='adam', loss='mean_absolute_error')

    #fit the model
    model.fit(X_train, y_train, epochs=50, verbose=0)

    #predict output on test set
    y_pred = model.predict(X_test)
    
    mae_A.append(mean_absolute_error(y_test, y_pred))
    r2_A.append(r2_score(y_test, y_pred))

print('mse_Mean: {:.2f}'.format(np.mean(mae_A)))
print('mse_StdDev: {:.2f}'.format(np.std(mae_A)))

print('R^2_Mean: {:.2f}'.format(np.mean(r2_A)))
print('R^2_StdDev: {:.2f}'.format(np.std(r2_A)))

#Building the Model

#Network Properties:(Hidden Layer: 1,Nodes: 10,Activation Function: ReLU,Optimizer: Adam,Loss Function: Mean absolute Error,Epochs: 50)

##Model is retrain with normalized data.

X_norm = (X - X.mean()) / X.std()
X_norm.head()

mae_B = []
r2_B = []

for i in range(50):
    
    #Split Data to Train and Test Set
    X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size = 0.3)

    #Create model
    model = Sequential()
    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))
    model.add(Dense(1))

    #Compile model
    model.compile(optimizer='adam', loss='mean_absolute_error')

    #fit the model
    model.fit(X_train, y_train, epochs=50, verbose=0)

    #predict output on test set
    y_pred = model.predict(X_test)
    
    mae_B.append(mean_absolute_error(y_test, y_pred))
    r2_B.append(r2_score(y_test, y_pred))

print('mae_Mean: {:.2f}'.format(np.mean(mae_B)))
print('mae_StdDev: {:.2f}'.format(np.std(mae_B)))

print('R^2_Mean: {:.2f}'.format(np.mean(r2_B)))
print('R^2_StdDev: {:.2f}'.format(np.std(r2_B)))

#Building the Model

#Network Properties:(Hidden Layer: 1,Nodes: 10,Activation Function: ReLU,Optimizer: Adam,Loss Function: Mean absolute Error,Epochs: 100)

##Model is retrain with 100 epochs.

mae_C = []
r2_C = []

for i in range(50):
    
    #Split Data to Train and Test Set
    X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size = 0.3)

    #Create model
    model = Sequential()
    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))
    model.add(Dense(1))

    #Compile model
    model.compile(optimizer='adam', loss='mean_absolute_error')

    #fit the model
    model.fit(X_train, y_train, epochs=100, verbose=0)

    #predict output on test set
    y_pred = model.predict(X_test)
    
    mae_C.append(mean_absolute_error(y_test, y_pred))
    r2_C.append(r2_score(y_test, y_pred))

print('mae_Mean: {:.2f}'.format(np.mean(mae_C)))
print('mae_StdDev: {:.2f}'.format(np.std(mae_C)))

print('R^2_Mean: {:.2f}'.format(np.mean(r2_C)))
print('R^2_StdDev: {:.2f}'.format(np.std(r2_C)))

#Building the Model

#Network Properties:(Hidden Layer: 3,Nodes: 10,Activation Function: ReLU,Optimizer: Adam,Loss Function: Mean absolute Error,Epochs: 100)

##Model is retrain with 3 hidden layers.

mae_D = []
r2_D = []

for i in range(50):
    
    #Split Data to Train and Test Set
    X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size = 0.3)

    #Create model
    model = Sequential()
    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(1))

    #Compile model
    model.compile(optimizer='adam', loss='mean_absolute_error')

    #fit the model
    model.fit(X_train, y_train, epochs=100, verbose=0)

    #predict output on test set
    y_pred = model.predict(X_test)
    
    mae_D.append(mean_absolute_error(y_test, y_pred))
    r2_D.append(r2_score(y_test, y_pred))

print('mae_Mean: {:.2f}'.format(np.mean(mae_D)))
print('mae_StdDev: {:.2f}'.format(np.std(mae_D)))

print('R^2_Mean: {:.2f}'.format(np.mean(r2_D)))
print('R^2_StdDev: {:.2f}'.format(np.std(r2_D)))

from IPython.display import HTML, display
import tabulate

tabletest = [['STEPS','MAE: Mean','MAE: StdDev','R^2: Mean','R^2: StdDev'],
         ['A', round(np.mean(mae_A),2), round(np.std(mae_A),2), round(np.mean(r2_A),2), round(np.std(r2_A),2)],
         ['B', round(np.mean(mae_B),2), round(np.std(mae_B),2), round(np.mean(r2_B),2), round(np.std(r2_B),2)],
         ['C', round(np.mean(mae_C),2), round(np.std(mae_C),2), round(np.mean(r2_D),2), round(np.std(r2_C),2)],
         ['D', round(np.mean(mae_D),2), round(np.std(mae_D),2), round(np.mean(r2_D),2), round(np.std(r2_D),2)]]

display(HTML(tabulate.tabulate(tabletest, tablefmt='html')))

#From the results above, we can clearly see that by applying:

#Data Normalization,
#Increasing Epochs,
#and Increasing Hidden Layers
#the mean of MSE has gone down, while the mean of R^2 has gone up indicating that the model accuracy is getting better.##

